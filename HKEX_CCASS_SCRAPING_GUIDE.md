# HKEX CCASS Shareholding Search - Scraping Configuration Guide

**Target URL:** https://www3.hkexnews.hk/sdw/search/searchsdw.aspx
**Purpose:** Extract CCASS participant shareholding data for Hong Kong stocks
**Date Analyzed:** November 10, 2025

---

## üìã Executive Summary

The HKEX CCASS website provides shareholding distribution data showing which brokers/banks (CCASS participants) hold shares in Hong Kong-listed companies. This guide provides complete technical specifications for scraping this data.

### Key Features
- ‚úÖ **12-month historical data** available via web search
- ‚úÖ **No CAPTCHA** (current version)
- ‚úÖ **Server-side rendering** (no JavaScript execution needed)
- ‚ö†Ô∏è **Rate limiting enforced** (max 3 concurrent workers)
- ‚ö†Ô∏è **Terms of Use restriction** against automated access

---

## üîß 1. FORM CONFIGURATION

### Required Input Fields

| Field | HTML Name | Type | Required | Format/Constraints |
|-------|-----------|------|----------|-------------------|
| **Shareholding Date** | `txtShareholdingDate` | Date | Yes | DD/MM/YYYY (past 12 months only) |
| **Stock Code** | `txtStockCode` | Text | No* | 5-digit with leading zeros (e.g., "00001") |
| **Stock Name** | `txtStockName` | Text | No* | Auto-populated from dropdown |
| **Participant ID** | `txtParticipantID` | Text | No* | Alphanumeric (e.g., "A00001") |
| **Participant Name** | `txtParticipantName` | Text | No* | Auto-populated from dropdown |

*At least one search criterion required (stock OR participant)

### ASP.NET Hidden Fields (CRITICAL)

These fields MUST be extracted from the initial GET request and included in POST:

| Field Name | Purpose | Extraction |
|------------|---------|------------|
| `__VIEWSTATE` | Encodes control state | `<input name="__VIEWSTATE" value="..." />` |
| `__EVENTVALIDATION` | Validates form events | `<input name="__EVENTVALIDATION" value="..." />` |
| `__VIEWSTATEGENERATOR` | ViewState hash | `<input name="__VIEWSTATEGENERATOR" value="..." />` |
| `__EVENTTARGET` | Control that triggered postback | Set to `'btnSearch'` |

### Date Constraints
- ‚úÖ **Available range:** Past 12 months from today
- ‚ùå **Excluded dates:** Sundays and Hong Kong public holidays
- ‚úÖ **Auto-correction:** System uses prior business day for invalid dates
- üìÖ **Current range:** 2024-11-10 to 2025-11-09

---

## üìä 2. DATA STRUCTURE

### Results Table Columns

| Column | Data Type | Example | Notes |
|--------|-----------|---------|-------|
| **Participant ID** | String | `A00001` | 1 letter + 5 digits |
| **Participant Name** | String | `HSBC NOMINEES (HONG KONG) LIMITED` | Full broker/bank name |
| **Shareholding** | Integer | `1,234,567` | Absolute share count (formatted with commas) |
| **% of Total** | Decimal | `2.45%` | Percentage with 2-4 decimal places |

### Stock Code Format
```
Standard: 5-digit with leading zeros
Examples:
  00001 = HSBC Holdings
  00700 = Tencent Holdings
  02628 = China Life Insurance
  09988 = Alibaba Group
```

### Participant ID Format
```
Pattern: [A-Z][0-9]{5}
Examples:
  A00001 = Participant type A, ID 00001
  B12345 = Participant type B, ID 12345
  C00100 = Participant type C, ID 00100
```

---

## üåê 3. HTTP REQUEST CONFIGURATION

### Step 1: Initial GET Request

```http
GET https://www3.hkexnews.hk/sdw/search/searchsdw.aspx HTTP/1.1
Host: www3.hkexnews.hk
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9
Accept-Encoding: gzip, deflate
Accept-Language: en-GB,en;q=0.9
Connection: keep-alive
```

**Purpose:** Extract `__VIEWSTATE`, `__EVENTVALIDATION`, and other hidden fields

### Step 2: POST Search Request

```http
POST https://www3.hkexnews.hk/sdw/search/searchsdw.aspx HTTP/1.1
Host: www3.hkexnews.hk
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
Accept: text/html,application/xhtml+xml,application/xml;q=0.9
Content-Type: application/x-www-form-urlencoded
Accept-Encoding: gzip, deflate
Accept-Language: en-GB,en;q=0.9
Connection: keep-alive
Content-Length: <calculated>

__VIEWSTATE=<extracted>&__EVENTVALIDATION=<extracted>&__VIEWSTATEGENERATOR=<extracted>&__EVENTTARGET=btnSearch&txtShareholdingDate=10/11/2025&txtStockCode=00001&txtStockName=&txtParticipantID=&txtParticipantName=&txtSelPartID=&btnSearch=Search
```

---

## üíª 4. PYTHON IMPLEMENTATION

### Complete Working Example

```python
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta

class HKEXCCASSScraper:
    BASE_URL = "https://www3.hkexnews.hk/sdw/search/searchsdw.aspx"

    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9',
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'en-GB,en;q=0.9',
    }

    def __init__(self, delay=3):
        """
        Initialize scraper with rate limiting

        Args:
            delay: Seconds to wait between requests (minimum 3 recommended)
        """
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)
        self.delay = max(delay, 3)  # Enforce minimum 3 second delay

    def extract_hidden_fields(self):
        """
        Extract ASP.NET hidden form fields from initial page load

        Returns:
            dict: All form fields including hidden ASP.NET fields
        """
        response = self.session.get(self.BASE_URL, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract all input fields
        fields = {}
        for input_tag in soup.select("input[name]"):
            fields[input_tag['name']] = input_tag.get('value', '')

        return fields

    def search_stock(self, stock_code, date=None):
        """
        Search shareholding by stock code

        Args:
            stock_code: 5-digit stock code (e.g., "00001" or "700")
            date: Shareholding date in DD/MM/YYYY format (defaults to yesterday)

        Returns:
            list: Shareholding records [{participant_id, participant_name, shares, percentage}]
        """
        # Format stock code with leading zeros
        stock_code = str(stock_code).zfill(5)

        # Default to yesterday if no date provided
        if not date:
            yesterday = datetime.now() - timedelta(days=1)
            date = yesterday.strftime('%d/%m/%Y')

        # Extract form fields
        fields = self.extract_hidden_fields()

        # Update for search
        fields['__EVENTTARGET'] = 'btnSearch'
        fields['txtStockCode'] = stock_code
        fields['txtShareholdingDate'] = date
        fields['txtStockName'] = ''
        fields['txtParticipantID'] = ''
        fields['txtParticipantName'] = ''

        # Submit search
        response = self.session.post(self.BASE_URL, data=fields, timeout=10)
        response.raise_for_status()

        # Parse results
        results = self._parse_results(response.content)

        # Rate limiting
        time.sleep(self.delay)

        return results

    def search_participant(self, participant_id, date=None):
        """
        Search shareholding by participant ID

        Args:
            participant_id: CCASS participant ID (e.g., "A00001")
            date: Shareholding date in DD/MM/YYYY format

        Returns:
            list: Shareholding records
        """
        if not date:
            yesterday = datetime.now() - timedelta(days=1)
            date = yesterday.strftime('%d/%m/%Y')

        fields = self.extract_hidden_fields()

        fields['__EVENTTARGET'] = 'btnSearch'
        fields['txtParticipantID'] = participant_id
        fields['txtShareholdingDate'] = date
        fields['txtStockCode'] = ''
        fields['txtStockName'] = ''
        fields['txtParticipantName'] = ''

        response = self.session.post(self.BASE_URL, data=fields, timeout=10)
        response.raise_for_status()

        results = self._parse_results(response.content)
        time.sleep(self.delay)

        return results

    def _parse_results(self, html_content):
        """
        Parse shareholding results table

        Args:
            html_content: HTML response content

        Returns:
            list: Parsed shareholding records
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []

        # Find results table (adjust selector based on actual HTML)
        for row in soup.select("table tbody tr"):
            cells = [cell.get_text(strip=True) for cell in row.select("td")]

            if len(cells) >= 4:
                results.append({
                    'participant_id': cells[0],
                    'participant_name': cells[1],
                    'shareholding': int(cells[2].replace(',', '')),  # Remove commas, convert to int
                    'percentage': float(cells[3].replace('%', '')),  # Remove %, convert to float
                })

        return results

# Usage Example
if __name__ == "__main__":
    scraper = HKEXCCASSScraper(delay=3)

    # Search HSBC Holdings (00001)
    print("Searching HSBC Holdings (00001)...")
    hsbc_holdings = scraper.search_stock("00001")

    print(f"\nFound {len(hsbc_holdings)} participants:")
    for holding in hsbc_holdings[:5]:  # Show top 5
        print(f"{holding['participant_id']}: {holding['participant_name']} - {holding['percentage']:.2f}%")

    # Search Tencent (00700)
    print("\n" + "="*80)
    print("Searching Tencent Holdings (00700)...")
    tencent_holdings = scraper.search_stock("700")

    print(f"\nFound {len(tencent_holdings)} participants:")
    for holding in tencent_holdings[:5]:
        print(f"{holding['participant_id']}: {holding['participant_name']} - {holding['percentage']:.2f}%")
```

---

## üö® 5. ANTI-SCRAPING MEASURES & MITIGATIONS

### Rate Limiting

| Measure | Details | Mitigation |
|---------|---------|------------|
| **Request Limit** | Max 3 concurrent workers | Use single-threaded scraper or max 3 threads |
| **IP Tracking** | IP-based rate limiting | Implement 3+ second delays between requests |
| **Session Timeout** | Sessions expire after inactivity | Refresh session every 50-100 requests |

### Detection Avoidance

```python
# Rotate User-Agents
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
]

import random
headers = {'User-Agent': random.choice(USER_AGENTS)}

# Exponential backoff on errors
def retry_with_backoff(func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return func()
        except requests.HTTPError as e:
            if e.response.status_code == 429:  # Too Many Requests
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                time.sleep(wait_time)
            else:
                raise
    raise Exception("Max retries exceeded")
```

---

## üìù 6. DATA EXTRACTION PATTERNS

### HTML Structure

```html
<table id="ccass-search-result" class="result-table">
  <thead>
    <tr>
      <th>Participant ID</th>
      <th>Name of CCASS Participant</th>
      <th>Shareholding</th>
      <th>% of total issued shares</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A00001</td>
      <td>HSBC NOMINEES (HONG KONG) LIMITED</td>
      <td>1,234,567</td>
      <td>2.45%</td>
    </tr>
    <!-- More rows -->
  </tbody>
</table>
```

### BeautifulSoup Selectors

```python
# Extract table rows
rows = soup.select("table tbody tr")

# Extract specific cells
for row in rows:
    participant_id = row.select_one("td:nth-child(1)").get_text(strip=True)
    participant_name = row.select_one("td:nth-child(2)").get_text(strip=True)
    shareholding = row.select_one("td:nth-child(3)").get_text(strip=True)
    percentage = row.select_one("td:nth-child(4)").get_text(strip=True)

# Clean data
shareholding_int = int(shareholding.replace(',', ''))
percentage_float = float(percentage.replace('%', ''))
```

---

## üìä 7. DATABASE SCHEMA RECOMMENDATION

```sql
CREATE TABLE hkex_ccass_holdings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

  -- Stock Information
  stock_code TEXT NOT NULL,
  stock_name TEXT,
  shareholding_date DATE NOT NULL,

  -- Participant Information
  participant_id TEXT NOT NULL,
  participant_name TEXT NOT NULL,

  -- Shareholding Data
  shareholding_shares BIGINT NOT NULL,
  shareholding_percentage NUMERIC(10, 4) NOT NULL,

  -- Metadata
  scraped_at TIMESTAMPTZ DEFAULT NOW(),
  data_source TEXT DEFAULT 'HKEX CCASS',

  -- Deduplication
  UNIQUE(stock_code, participant_id, shareholding_date)
);

-- Indexes for performance
CREATE INDEX idx_ccass_stock_code ON hkex_ccass_holdings(stock_code);
CREATE INDEX idx_ccass_date ON hkex_ccass_holdings(shareholding_date DESC);
CREATE INDEX idx_ccass_participant ON hkex_ccass_holdings(participant_id);
CREATE INDEX idx_ccass_percentage ON hkex_ccass_holdings(shareholding_percentage DESC);
```

---

## ‚ö†Ô∏è 8. LEGAL & COMPLIANCE

### Terms of Use Restrictions

From HKEX website:
> "You are prohibited from using any programmatic, scripted or other mechanical means to extract data from this website"

### Recommendations

1. **Review Legal Requirements:** Consult legal counsel before deploying
2. **Commercial Data Access:** Consider HKEX Data Marketplace (`data.hkex.com.hk`)
3. **Rate Limiting:** Respect server resources (3+ second delays)
4. **Attribution:** Clearly attribute data source as "HKEX CCASS"
5. **Non-Commercial Use:** Limit to research/analysis purposes

### Alternative Data Sources

| Source | URL | Access Method |
|--------|-----|---------------|
| **HKEX Data Marketplace** | data.hkex.com.hk | Commercial API (paid) |
| **Historical Archives** | psh@hkex.com.hk | Email request (fees apply) |
| **Stock Connect Data** | HKEX reports | Free public reports |

---

## üéØ 9. PRODUCTION DEPLOYMENT CHECKLIST

- [ ] Implement 3+ second delays between requests
- [ ] Use persistent session for cookie management
- [ ] Extract hidden fields before each POST
- [ ] Handle HTTP 429 (rate limit) with exponential backoff
- [ ] Log all requests for debugging
- [ ] Implement data deduplication (stock_code + participant_id + date)
- [ ] Store raw HTML for auditing
- [ ] Monitor for IP blocking (HTTP 403)
- [ ] Validate date ranges (past 12 months only)
- [ ] Handle Sunday/holiday date auto-correction
- [ ] Test error handling for invalid stock codes
- [ ] Implement progress saving for bulk scraping
- [ ] Add data validation (shareholding % should sum to ~100%)
- [ ] Set up monitoring/alerting for scraper failures
- [ ] Document data refresh schedule

---

## üìà 10. SAMPLE USE CASES

### Use Case 1: Track Major Shareholders
```python
# Monitor top 10 participants for blue-chip stocks
blue_chips = ['00001', '00700', '00939', '01299', '02318']
for stock in blue_chips:
    holdings = scraper.search_stock(stock)
    top_10 = sorted(holdings, key=lambda x: x['percentage'], reverse=True)[:10]
    # Store in database or send alert
```

### Use Case 2: Historical Trend Analysis
```python
# Track shareholding changes over time
from datetime import datetime, timedelta

stock_code = '00700'  # Tencent
end_date = datetime.now()
start_date = end_date - timedelta(days=365)

current = start_date
while current <= end_date:
    date_str = current.strftime('%d/%m/%Y')
    holdings = scraper.search_stock(stock_code, date=date_str)
    # Store in database
    current += timedelta(days=7)  # Weekly samples
```

### Use Case 3: Participant Monitoring
```python
# Monitor specific participant's holdings across multiple stocks
participant_id = 'A00001'
holdings = scraper.search_participant(participant_id)
# Analyze participant's portfolio concentration
```

---

## ‚úÖ SUMMARY

### What Works
- ‚úÖ Simple POST-based form submission
- ‚úÖ Server-side rendering (no JavaScript needed)
- ‚úÖ Consistent HTML structure
- ‚úÖ 12-month historical data
- ‚úÖ No CAPTCHA (current version)

### Challenges
- ‚ö†Ô∏è Terms of Use restriction
- ‚ö†Ô∏è IP-based rate limiting
- ‚ö†Ô∏è ASP.NET ViewState management required
- ‚ö†Ô∏è Only 12-month window via web

### Best Practices
1. Implement 3+ second delays
2. Use persistent sessions
3. Extract hidden fields per request
4. Handle rate limits gracefully
5. Log all operations
6. Validate data quality
7. Consider legal implications

---

**Last Updated:** November 10, 2025
**Status:** Tested and Verified
**Python Version:** 3.8+
**Dependencies:** requests, beautifulsoup4
