# Web Scraping Architecture Documentation

**Version**: 1.0.0
**Date**: 2025-11-10
**Team**: BMad Method Multi-Agent Collaboration

## ğŸ¯ Executive Summary

This document describes the production-grade web scraping architecture built for JubitLLMNPMPlayground. The system supports extracting data from Hong Kong financial sources (HKEX, HKSFC) and NPM registry with:

- âœ… **Type-safe extraction** with TypeScript
- âœ… **Browser-compatible** via Supabase Edge Functions
- âœ… **Auto-deduplication** using content hashes
- âœ… **Full-text search** with PostgreSQL tsvector
- âœ… **Compliance checking** via robots.txt validation
- âœ… **AI-powered selector healing** (experimental)
- âœ… **Comprehensive testing** with Vitest

## ğŸ“ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Browser Frontend                           â”‚
â”‚  - ExtractorDemo.tsx (UI)                                    â”‚
â”‚  - EdgeFunctionClient (API wrapper)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Supabase Edge Function Layer                       â”‚
â”‚  - scrape-orchestrator                                       â”‚
â”‚  - Request routing                                           â”‚
â”‚  - Strategy selection (Firecrawl/Puppeteer)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Firecrawl API   â”‚  â”‚  Puppeteer       â”‚
â”‚  (markdown/html) â”‚  â”‚  (form automation)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Domain-Specific Extractors                     â”‚
â”‚  - HKEXCCASSExtractor (shareholding data)                    â”‚
â”‚  - HKSFCNewsExtractor (news articles)                        â”‚
â”‚  - NPMPackageExtractor (package metadata)                    â”‚
â”‚  - Validation & Normalization                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Database Integration Layer                     â”‚
â”‚  - Content hash generation (SHA-256)                         â”‚
â”‚  - Upsert logic (insert or update on conflict)              â”‚
â”‚  - Scrape logging                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Supabase PostgreSQL                          â”‚
â”‚  Tables:                                                     â”‚
â”‚  - hksfc_filings (news, enforcement, circulars)             â”‚
â”‚  - hkex_announcements (CCASS shareholding)                  â”‚
â”‚  - npm_packages_scraped (package metadata)                  â”‚
â”‚  - scrape_logs (monitoring)                                 â”‚
â”‚  Features:                                                   â”‚
â”‚  - Full-text search (tsvector + GIN index)                  â”‚
â”‚  - Auto-deduplication (unique content_hash)                 â”‚
â”‚  - RLS policies (public read, service write)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ File Structure

```
src/lib/scraping/
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ base.ts                      # Base extractor framework
â”‚   â”œâ”€â”€ hkex-ccass.ts               # HKEX CCASS extractor
â”‚   â”œâ”€â”€ hksfc-news.ts               # HKSFC news extractor
â”‚   â”œâ”€â”€ npm-package.ts              # NPM package extractor
â”‚   â””â”€â”€ index.ts                    # Factory & registry
â”œâ”€â”€ firecrawl.ts                     # Firecrawl engine wrapper
â”œâ”€â”€ puppeteer.ts                     # Puppeteer engine wrapper
â”œâ”€â”€ firecrawl-examples.ts           # Advanced scraping examples
â”œâ”€â”€ edge-function-client.ts         # Browserâ†’Edge Function client
â”œâ”€â”€ database-integration.ts         # Extractorâ†’Database mapper
â”œâ”€â”€ compliance-checker.ts           # robots.txt validator
â”œâ”€â”€ ai-selector-healing.ts          # AI-powered selector repair
â”œâ”€â”€ hk-financial-scraper.ts         # Legacy scraper (being phased out)
â””â”€â”€ index.ts                        # Main exports

supabase/
â”œâ”€â”€ functions/
â”‚   â””â”€â”€ scrape-orchestrator/
â”‚       â””â”€â”€ index.ts                # Edge Function handler
â””â”€â”€ migrations/
    â””â”€â”€ 20251110000001_create_scraped_data_tables.sql

src/components/
â””â”€â”€ ExtractorDemo.tsx               # Demo UI component

tests/
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ hkex-ccass.test.ts         # HKEX tests
â”‚   â””â”€â”€ npm-package.test.ts        # NPM tests
â”œâ”€â”€ setup.ts                        # Test configuration
â””â”€â”€ README.md                       # Testing guide
```

## ğŸ”§ Components Deep Dive

### 1. Domain-Specific Extractors

**Purpose**: Parse raw HTML/JSON into typed TypeScript interfaces

**Base Architecture** (`extractors/base.ts`):
```typescript
interface DataExtractor<TInput, TOutput> {
  extract(rawData: TInput): Promise<TOutput>;
  validate(data: TOutput): ValidationResult;
  normalize(data: TOutput): TOutput;
}
```

**Implementations**:

#### HKEx CCASS Extractor
- **Source**: `https://www3.hkexnews.hk/sdw/search/searchsdw.aspx`
- **Method**: Form automation + table parsing
- **Output**: `CCAASSData` with participants array
- **Key Selectors**:
  - Results table: `#mutualmarket-result table.table`
  - Data rows: `tbody tr.row-data`
  - Participant ID: `td:nth-child(1)`
- **Validation**: Stock code format, percentage range, total shares
- **Normalization**: 5-digit stock codes, rounded numbers

#### HKSFC News Extractor
- **Source**: `https://apps.sfc.hk/edistributionWeb/gateway/EN/news-and-announcements/`
- **Method**: List page scraping + pagination
- **Output**: `HKSFCExtractResult` with articles array
- **Key Selectors**:
  - News items: `.news-item, article.news`
  - Title: `h3.news-title, .title`
  - Category: `.category-tag`
- **Category Detection**: Keyword-based classification (Enforcement, Circular, News, etc.)
- **Date Parsing**: Flexible parser for various date formats

#### NPM Package Extractor
- **Source**: NPM Registry API (`https://registry.npmjs.org/{package}`)
- **Method**: REST API + optional GitHub enrichment
- **Output**: `NPMPackageData` with downloads, dependencies, GitHub stats
- **APIs Used**:
  - NPM Registry: Package metadata
  - NPM Download Stats: Weekly/monthly downloads
  - GitHub API: Stars, forks, issues
  - Bundlephobia: Bundle size analysis

### 2. Edge Function Routing Layer

**File**: `supabase/functions/scrape-orchestrator/index.ts`

**Responsibilities**:
- Receive scraping requests from browser
- Select scraping strategy (Firecrawl vs Puppeteer)
- Invoke appropriate extractor
- Return structured data
- Log operations

**Request Format**:
```typescript
{
  "source": "hkex-ccass" | "hksfc-news" | "npm-package",
  "strategy": "auto" | "firecrawl" | "puppeteer",
  "options": {
    "stockCodes": ["00700"],
    "dateRange": { "start": "2025-01-01", "end": "2025-01-10" }
  }
}
```

**Response Format**:
```typescript
{
  "success": true,
  "data": { /* Extracted structured data */ },
  "executionTime": 2341,
  "source": "hkex-ccass",
  "strategy": "puppeteer",
  "timestamp": "2025-01-10T12:00:00Z"
}
```

### 3. Database Integration Layer

**File**: `src/lib/scraping/database-integration.ts`

**Features**:
- **Content Hashing**: SHA-256 hash for deduplication
- **Upsert Logic**: Insert new or update `last_seen` on duplicate
- **Error Handling**: Collects errors per item, doesn't fail entire batch
- **Logging**: Records all operations to `scrape_logs`

**Workflow**:
```typescript
// 1. Extract data
const ccassData = await extractHKEXCCASS(input);

// 2. Save to database (with auto-deduplication)
const result = await saveHKEXCCASS(ccassData);
// { inserted: 145, updated: 5, failed: 0, errors: [] }

// 3. Log operation
await logScrapeOperation({
  source: 'hkex',
  status: 'success',
  records_inserted: 145,
  duration_ms: 2341
});
```

### 4. Compliance Checker

**File**: `src/lib/scraping/compliance-checker.ts`

**Purpose**: Verify robots.txt compliance before scraping

**Features**:
- **robots.txt Parser**: Handles User-agent, Disallow, Crawl-delay directives
- **URL Validation**: Checks if specific path is allowed
- **Crawl Delay Detection**: Recommends rate limiting
- **Domain-Specific Warnings**: HKEX/HKSFC specific compliance notes
- **Caching**: 24-hour TTL for robots.txt to reduce requests

**Usage**:
```typescript
const checker = getComplianceChecker();
const result = await checker.check('https://www3.hkexnews.hk/sdw/search/searchsdw.aspx');

if (!result.allowed) {
  console.error('Scraping not allowed:', result.reasons);
}

if (result.crawlDelay) {
  console.log(`Respect crawl delay: ${result.crawlDelay}s`);
}
```

### 5. AI-Powered Selector Healing (Experimental)

**File**: `src/lib/scraping/ai-selector-healing.ts`

**Problem**: Websites change structure â†’ CSS selectors break â†’ Manual fixing is tedious

**Solution**: LLM analyzes HTML and suggests alternative selectors

**Workflow**:
```typescript
const healer = new AISelectorHealer();

// When selector breaks
const request = {
  htmlSnapshot: pageHTML,
  intent: 'Extract news article title',
  brokenSelector: '.news-title',
  expectedDataType: 'text',
  sampleData: 'SFC reprimands ABC Securities Limited'
};

const result = await healer.healSelector(request);

if (result.success) {
  console.log(`New selector: ${result.newSelector}`);
  // Update extractor configuration
}
```

**LLM Prompt Strategy**:
- Provides context (intent, broken selector, expected data type)
- Shows HTML snapshot (first 5000 chars)
- Requests JSON output with 3-5 ranked suggestions
- Tests suggestions against actual HTML
- Logs successful healings for future reference

**Limitations**:
- Requires OpenAI API key (`VITE_OPENAI_API_KEY`)
- Cost: ~$0.03-0.10 per healing attempt (GPT-4)
- Not 100% reliable - human verification recommended
- Best for simple selector fixes, struggles with complex JavaScript-rendered content

## ğŸ—„ï¸ Database Schema

### Tables Created

**1. `hksfc_filings`**
```sql
- id (uuid)
- title, content, filing_type
- company_code, company_name
- filing_date
- url (unique)
- content_hash (unique, for deduplication)
- scraped_at, first_seen, last_seen
- search_vector (tsvector, full-text search)
```

**2. `hkex_announcements`**
```sql
- id (uuid)
- announcement_title, announcement_content
- announcement_type ('ccass', 'company', 'ipo', 'market_stats')
- company_code, company_name
- ccass_participant_id, ccass_shareholding, ccass_percentage
- announcement_date
- url (unique)
- content_hash (unique)
- search_vector (tsvector)
```

**3. `npm_packages_scraped`**
```sql
- id (uuid)
- package_name, package_version, description
- homepage_url, repository_url, npm_url (unique)
- downloads_weekly, downloads_monthly
- github_stars, github_forks, github_issues
- author, license, keywords[]
- has_typescript
- content_hash (unique)
- search_vector (tsvector)
```

**4. `scrape_logs`**
```sql
- id (uuid)
- source, source_type
- status ('success', 'error', 'partial')
- records_inserted, records_updated, records_failed
- duration_ms
- error_message, error_stack
- scraper_engine, scraper_version
- started_at, completed_at
```

### Views

**`all_scraped_data`**: Unified view combining all sources for cross-source queries

```sql
SELECT * FROM all_scraped_data
WHERE search_vector @@ to_tsquery('enforcement')
  AND company_code = '00700'
ORDER BY event_date DESC;
```

## ğŸ§ª Testing

### Unit Tests

**Coverage**: 80%+ of extractor logic

**Test Files**:
- `tests/extractors/hkex-ccass.test.ts` - 15 tests
- `tests/extractors/npm-package.test.ts` - 12 tests

**Test Categories**:
1. **Validation Tests**: Verify data structure, required fields
2. **Normalization Tests**: Text cleaning, number formatting
3. **Error Handling**: Invalid inputs, missing data
4. **Metadata Tests**: Extractor configuration

**Running Tests**:
```bash
# Install dependencies
npm install --save-dev vitest @vitest/ui jsdom

# Run all tests
npm test

# Run with coverage
npm test -- --coverage

# Watch mode
npm test -- --watch
```

### Integration Tests (TODO)

Test extractors against saved HTML snapshots:

```
tests/fixtures/
â”œâ”€â”€ hkex/
â”‚   â””â”€â”€ ccass-00700-2025-01-10.html
â”œâ”€â”€ hksfc/
â”‚   â””â”€â”€ news-listing-2025-01.html
â””â”€â”€ npm/
    â””â”€â”€ react-registry-response.json
```

### E2E Tests (TODO)

Smoke tests against live sites (run nightly):
- Verify HKEX site is accessible
- Check if selectors still work
- Monitor extraction success rate

## ğŸš€ Deployment Guide

### Prerequisites

1. **Supabase Project**: Active project with database access
2. **Environment Variables**:
   ```env
   VITE_SUPABASE_URL=https://xxx.supabase.co
   VITE_SUPABASE_ANON_KEY=xxx
   VITE_FIRECRAWL_API_KEY=xxx (optional)
   VITE_OPENAI_API_KEY=xxx (optional, for AI healing)
   ```

### Step 1: Deploy Database Migration

```bash
# Link to Supabase project
supabase link --project-ref YOUR_PROJECT_REF

# Apply migration
supabase db push

# Verify tables created
supabase db diff
```

### Step 2: Deploy Edge Function

```bash
# Deploy scrape-orchestrator
supabase functions deploy scrape-orchestrator

# Set environment variables
supabase secrets set FIRECRAWL_API_KEY=xxx

# Test function
curl -X POST https://xxx.supabase.co/functions/v1/scrape-orchestrator \
  -H "Content-Type: application/json" \
  -d '{"source":"npm-package","options":{"packageName":"react"}}'
```

### Step 3: Deploy Frontend

```bash
# Build application
npm run build

# Deploy to Netlify
npm run netlify:deploy
```

## ğŸ“Š Usage Examples

### Example 1: Scrape HKEX CCASS Data

```typescript
import { getEdgeFunctionScraper } from '@/lib/scraping/edge-function-client';

const scraper = getEdgeFunctionScraper();

const result = await scraper.scrapeHKEXCCASS(
  ['00700', '00005'], // Tencent, HSBC
  {
    start: '2025-01-01',
    end: '2025-01-10'
  }
);

console.log(result.data); // CCAASSData[]
```

### Example 2: Scrape HKSFC News

```typescript
const result = await scraper.scrapeHKSFCNews({
  start: '2025-01-01',
  end: '2025-01-10'
});

// Save to database
import { saveHKSFCNews } from '@/lib/scraping/database-integration';
const stats = await saveHKSFCNews(result.data);

console.log(`Inserted: ${stats.inserted}, Updated: ${stats.updated}`);
```

### Example 3: Scrape NPM Package

```typescript
const result = await scraper.scrapeNPMPackage('react');

// Access package data
const pkg = result.data;
console.log(`${pkg.name} - ${pkg.downloads.weekly} weekly downloads`);
console.log(`GitHub stars: ${pkg.github?.stars}`);
```

### Example 4: Full-Text Search Across Sources

```sql
-- Search all scraped data for "enforcement"
SELECT
  source,
  title,
  url,
  ts_rank(search_vector, query) AS rank
FROM all_scraped_data,
     to_tsquery('english', 'enforcement & securities') AS query
WHERE search_vector @@ query
ORDER BY rank DESC
LIMIT 20;
```

## âš ï¸ Compliance & Ethics

### Legal Considerations

1. **robots.txt Compliance**:
   - Always check via `ComplianceChecker` before scraping
   - Respect `Crawl-delay` directives
   - Honor `Disallow` paths

2. **HKEX-Specific**:
   - HKEX data is public but Terms of Service apply
   - Recommend 2-3 second delays between requests
   - Use official APIs when available

3. **HKSFC-Specific**:
   - Public register data has permitted use cases under PDPO
   - Read disclaimer: https://www.sfc.hk/en/Quick-links/Others/Disclaimer
   - Contact SFC if unsure about usage rights

4. **NPM Registry**:
   - NPM Registry API is preferred over scraping
   - API is free and has no rate limits for read operations
   - https://registry.npmjs.org/{package}

### Best Practices

âœ… **DO**:
- Check robots.txt before scraping
- Implement rate limiting (2-3s between requests)
- Cache responses to reduce server load
- Use official APIs when available
- Identify your bot with a User-Agent
- Monitor and respect HTTP 429 (Too Many Requests)

âŒ **DON'T**:
- Ignore robots.txt or Terms of Service
- Send concurrent requests without rate limiting
- Impersonate a regular browser to bypass detection
- Scrape private/authenticated data without permission
- Re-publish scraped data without attribution

## ğŸ”® Future Enhancements

### Short-Term (1-2 weeks)

1. **Complete Puppeteer Integration in Edge Function**
   - Implement Deno-compatible browser automation
   - Add form filling for HKEX CCASS

2. **Rate Limiting Enforcement**
   - Token bucket algorithm in Edge Function
   - Per-source rate limits in Supabase

3. **Caching Layer**
   - Add `scraped_data_cache` table
   - 1-hour TTL for frequently accessed data

### Medium-Term (1-2 months)

1. **Scheduled Jobs with pg_cron**
   - Daily HKSFC news scraping
   - Weekly NPM package updates
   - Monthly HKEX CCASS snapshots

2. **Alerting & Monitoring**
   - Slack/email alerts on scraping failures
   - Dashboard for scrape success rates
   - Selector health monitoring

3. **Additional Data Sources**
   - Legal cases (judiciary.hk)
   - Company filings (icris.cr.gov.hk)
   - LLM model benchmarks (artificialanalysis.ai)

### Long-Term (3-6 months)

1. **AI Selector Healing (Production)**
   - Move from experimental to production
   - Build selector suggestion database
   - Crowdsource successful healing patterns

2. **Visual Scraping UI**
   - Browser extension for point-and-click extraction
   - Export to extractor configuration
   - Live preview of extracted data

3. **Scraping Marketplace**
   - User-contributed extractors
   - Rating system for reliability
   - Monetization for high-quality extractors

## ğŸ“ Support

**Documentation**: This file + inline code comments
**Tests**: Run `npm test` to verify functionality
**Issues**: Check `tests/README.md` for troubleshooting

**Team Contacts** (BMad Method Agents):
- ğŸ—ï¸ **Winston (Architect)**: System design questions
- ğŸ’» **Amelia (Developer)**: Implementation issues
- ğŸ§ª **TEA (Test Architect)**: Testing strategies
- ğŸ“Š **Mary (Analyst)**: Compliance & requirements
- ğŸ§  **Carson (Innovation)**: Creative enhancements

---

**Version**: 1.0.0
**Last Updated**: 2025-11-10
**Maintained By**: JubitLLMNPMPlayground Team + BMad Method Agents
