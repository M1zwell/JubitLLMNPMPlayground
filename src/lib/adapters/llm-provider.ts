import { v4 as uuidv4 } from 'uuid';

/**
 * LLM call parameters
 */
export interface LLMCallParams {
  model: string;
  prompt: string;
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
  stream?: boolean;
}

/**
 * LLM response structure
 */
export interface LLMResponse {
  content: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
  };
  metadata?: Record<string, any>;
}

/**
 * LLM model configuration
 */
export interface ModelConfig {
  id: string;
  name: string;
  provider: string;
  capabilities: string[];
  maxContextTokens: number;
  pricing: {
    input: number;  // per million tokens
    output: number; // per million tokens
  };
}

/**
 * Base interface for all LLM providers
 */
export interface LLMProvider {
  readonly name: string;
  readonly models: ModelConfig[];
  
  /**
   * Call the LLM model and get a complete response
   */
  call(params: LLMCallParams): Promise<LLMResponse>;
  
  /**
   * Stream responses from the LLM model
   */
  stream?(params: LLMCallParams): AsyncIterableIterator<string>;
}

/**
 * Mock LLM Provider for development/testing
 */
export class MockLLMProvider implements LLMProvider {
  readonly name = 'mock';
  
  readonly models = [
    {
      id: 'gpt-4o-mock',
      name: 'GPT-4o (Mock)',
      provider: 'OpenAI',
      capabilities: ['reasoning', 'coding', 'vision'],
      maxContextTokens: 128000,
      pricing: {
        input: 5.00,
        output: 15.00
      }
    },
    {
      id: 'claude-3-5-sonnet-mock',
      name: 'Claude 3.5 Sonnet (Mock)',
      provider: 'Anthropic',
      capabilities: ['reasoning', 'coding'],
      maxContextTokens: 200000,
      pricing: {
        input: 3.00,
        output: 15.00
      }
    }
  ];
  
  async call(params: LLMCallParams): Promise<LLMResponse> {
    // Simulate processing time
    await this.sleep(1000);
    
    const modelName = params.model.toLowerCase();
    const prompt = params.prompt.toLowerCase();
    
    // Generate appropriate mock response based on prompt content
    let responseText = '';
    if (prompt.includes('hello') || prompt.includes('hi')) {
      responseText = 'Hello! How can I assist you today?';
    }
    else if (prompt.includes('code') || prompt.includes('function') || prompt.includes('javascript')) {
      responseText = `\`\`\`javascript
function processData(input) {
  // This is mock code generation
  return input.map(item => ({
    ...item,
    processed: true,
    timestamp: new Date().toISOString()
  }));
}
\`\`\`

This function takes an array of items and adds processed status and timestamp to each item.`;
    }
    else if (prompt.includes('explain') || prompt.includes('what is')) {
      responseText = `Here's a detailed explanation:\n\n1. First, it's important to understand the context.\n2. The key concepts involve several interrelated factors.\n3. When applying this knowledge, consider the practical implications.\n\nThis is a mock explanation generated for demonstration purposes.`;
    }
    else if (prompt.includes('analyze') || prompt.includes('data')) {
      responseText = `# Data Analysis\n\nBased on the provided information, here are the key insights:\n\n- The primary trend shows an upward trajectory\n- Several anomalies were detected in the third segment\n- Correlation between variables A and B is strong (r=0.87)\n\nThis is a mock analysis generated for demonstration purposes.`;
    }
    else {
      responseText = `I've processed your request about "${params.prompt.substring(0, 30)}..."\n\nHere's my response as a ${params.model} model. This is a mock response for demonstration purposes. In a real implementation, this would be generated by the actual LLM API.`;
    }
    
    // Calculate token estimates (very rough approximation)
    const promptTokens = Math.ceil(params.prompt.length / 4);
    const completionTokens = Math.ceil(responseText.length / 4);
    
    return {
      content: responseText,
      usage: {
        promptTokens,
        completionTokens
      },
      metadata: {
        model: params.model,
        temperature: params.temperature || 0.7,
        mockResponse: true
      }
    };
  }
  
  async *stream(params: LLMCallParams): AsyncIterableIterator<string> {
    const response = await this.call(params);
    const chunks = this.chunkString(response.content, 10);
    
    for (const chunk of chunks) {
      await this.sleep(50); // Simulate streaming delay
      yield chunk;
    }
  }
  
  private chunkString(str: string, size: number): string[] {
    const chunks = [];
    for (let i = 0; i < str.length; i += size) {
      chunks.push(str.substring(i, i + size));
    }
    return chunks;
  }
  
  private async sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

/**
 * OpenAI LLM Provider
 * (Mock implementation - in production, this would use the actual OpenAI API)
 */
export class OpenAIProvider extends MockLLMProvider {
  readonly name = 'openai';
  
  readonly models = [
    {
      id: 'gpt-4o',
      name: 'GPT-4o',
      provider: 'OpenAI',
      capabilities: ['reasoning', 'coding', 'vision'],
      maxContextTokens: 128000,
      pricing: {
        input: 5.00,
        output: 15.00
      }
    },
    {
      id: 'gpt-4o-mini',
      name: 'GPT-4o Mini',
      provider: 'OpenAI',
      capabilities: ['reasoning', 'coding'],
      maxContextTokens: 128000,
      pricing: {
        input: 0.15,
        output: 0.60
      }
    }
  ];
  
  constructor(private apiKey: string) {
    super();
    
    // In a real implementation, we would validate the API key
  }
}

/**
 * Anthropic LLM Provider
 * (Mock implementation - in production, this would use the actual Anthropic API)
 */
export class AnthropicProvider extends MockLLMProvider {
  readonly name = 'anthropic';
  
  readonly models = [
    {
      id: 'claude-3-5-sonnet',
      name: 'Claude 3.5 Sonnet',
      provider: 'Anthropic',
      capabilities: ['reasoning', 'coding'],
      maxContextTokens: 200000,
      pricing: {
        input: 3.00,
        output: 15.00
      }
    },
    {
      id: 'claude-3-opus',
      name: 'Claude 3 Opus',
      provider: 'Anthropic',
      capabilities: ['reasoning', 'coding', 'analysis'],
      maxContextTokens: 180000,
      pricing: {
        input: 15.00,
        output: 75.00
      }
    },
    {
      id: 'claude-3-5-haiku',
      name: 'Claude 3.5 Haiku',
      provider: 'Anthropic',
      capabilities: ['reasoning', 'coding'],
      maxContextTokens: 200000,
      pricing: {
        input: 0.25,
        output: 1.25
      }
    }
  ];
  
  constructor(private apiKey: string) {
    super();
    
    // In a real implementation, we would validate the API key
  }
}

/**
 * LLM Provider Factory
 */
export class LLMProviderFactory {
  /**
   * Create a provider instance based on the provider name and API key
   */
  static create(providerName: string, apiKey: string): LLMProvider {
    switch (providerName.toLowerCase()) {
      case 'openai':
        return new OpenAIProvider(apiKey);
      case 'anthropic':
        return new AnthropicProvider(apiKey);
      case 'mock':
        return new MockLLMProvider();
      default:
        throw new Error(`Unsupported provider: ${providerName}`);
    }
  }
  
  /**
   * Get all available providers
   */
  static getAvailableProviders(): string[] {
    return ['openai', 'anthropic', 'mock'];
  }
}